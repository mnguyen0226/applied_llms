{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43023d7e",
   "metadata": {},
   "source": [
    "# L1 Language Models, the Chat Format and Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb0c943f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in /home/mnguyen0226/anaconda3/lib/python3.9/site-packages (0.7.0)\n",
      "Requirement already satisfied: requests>=2.26.0 in /home/mnguyen0226/anaconda3/lib/python3.9/site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/mnguyen0226/anaconda3/lib/python3.9/site-packages (from tiktoken) (2024.5.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/mnguyen0226/anaconda3/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken) (2024.6.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/mnguyen0226/anaconda3/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken) (3.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/mnguyen0226/anaconda3/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/mnguyen0226/anaconda3/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken) (1.26.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb393b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import tiktoken\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "openai.api_key  = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8381921f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    \"\"\"Function to get the first response as the user\"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0, # this is the degree of randomness of the model's output \n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bb07b0",
   "metadata": {},
   "source": [
    "### Prompt the model and get a completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77f940f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get_completion(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8a6ffab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c2337c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-9pfE3zsptFk7pFjAwveljzy2QvQEA at 0x75348a8ae270> JSON: {\n",
       "  \"id\": \"chatcmpl-9pfE3zsptFk7pFjAwveljzy2QvQEA\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"created\": 1722100227,\n",
       "  \"model\": \"gpt-3.5-turbo-0125\",\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"role\": \"assistant\",\n",
       "        \"content\": \"The capital of France is Paris.\"\n",
       "      },\n",
       "      \"logprobs\": null,\n",
       "      \"finish_reason\": \"stop\"\n",
       "    }\n",
       "  ],\n",
       "  \"usage\": {\n",
       "    \"prompt_tokens\": 14,\n",
       "    \"completion_tokens\": 7,\n",
       "    \"total_tokens\": 21\n",
       "  },\n",
       "  \"system_fingerprint\": null\n",
       "}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_completion_analyze(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    \"\"\"Function to get the first response as the user\"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0, # this is the degree of randomness of the model's output \n",
    "    )\n",
    "    return response\n",
    "\n",
    "responses = get_completion_analyze(prompt=\"What is the capital of France?\")\n",
    "responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d175b1",
   "metadata": {},
   "source": [
    "### Tokens\n",
    "- Why does it give the wrong answer? Because it does not predict the next word but the next token!\n",
    "- Why it works? Because it tokenize and better see the individual letter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "466c1e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pilpolol\n"
     ]
    }
   ],
   "source": [
    "response = get_completion(\"Take the letters in lollipop and reverse them\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d920834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The tricks is to add dashs to make it into tokens\n",
    "response = get_completion(\"\"\"Take the letters in \\\n",
    "l-o-l-l-i-p-o-p and reverse them\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "acc12b6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'p-o-p-i-l-l-o-l'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa08e68",
   "metadata": {},
   "source": [
    "### Let's experience with the usage of LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1020abad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion_from_messages(messages, \n",
    "                                 model=\"gpt-3.5-turbo\", \n",
    "                                 temperature=0, \n",
    "                                 max_tokens=500):\n",
    "    \"\"\"Function that allows the randomness of the model's output and max number of tokens\"\"\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature, # this is the degree of randomness of the model's output\n",
    "        max_tokens=max_tokens, # the maximum number of tokens the model can ouptut \n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb7b0d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh, the happy carrot in the ground so deep,\n",
      "With its orange hue, it makes me leap!\n",
      "In the garden, it grows so fine,\n",
      "Bringing joy with each veggie vine.\n",
      "So crunchy, so sweet, it's always a delight,\n",
      "The happy carrot, shining bright!\n"
     ]
    }
   ],
   "source": [
    "# Mock the role of the system, then ask the prompt!\n",
    "messages =  [  \n",
    "{'role':'system', \n",
    " 'content':\"\"\"You are an assistant who\\\n",
    " responds in the style of Dr Seuss.\"\"\"},    \n",
    "{'role':'user', \n",
    " 'content':\"\"\"write me a very short poem\\\n",
    " about a happy carrot\"\"\"},  \n",
    "] \n",
    "response = get_completion_from_messages(messages, temperature=1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7318c821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As of my last knowledge update, there are several open-source LLMs available for use. Some popular ones include OpenAI's GPT (Generative Pre-trained Transformer) models, Hugging Face's Transformers library, and EleutherAI's GPT-Neo. These models are widely used in natural language processing tasks and are continuously being developed and improved by the open-source community. It's always a good idea to check the latest repositories and resources to stay up-to-date on the available open-source LLMs.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {'role': 'system',\n",
    "     'content': \"You are a software engineer at BlackRock who are specialized in LLMs\"},\n",
    "    {'role': 'user',\n",
    "     'content': \"How many open sources LLMs are there?\"}\n",
    "]\n",
    "response = get_completion_from_messages(messages, temperature=0)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62f5628d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once there was a cheerful carrot named Charlie who brightened everyone's day with his radiant smile.\n"
     ]
    }
   ],
   "source": [
    "# length\n",
    "messages =  [  \n",
    "{'role':'system',\n",
    " 'content':'All your responses must be one sentence long.'},    \n",
    "{'role':'user',\n",
    " 'content':'write me a story about a happy carrot'},  \n",
    "] \n",
    "response = get_completion_from_messages(messages, temperature =1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc5f67d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a garden so bright, there lived a carrot so slight, who danced with delight in the sun's warm light.\n"
     ]
    }
   ],
   "source": [
    "# combined\n",
    "messages =  [  \n",
    "{'role':'system',\n",
    " 'content':\"\"\"You are an assistant who responds in the style of Dr Seuss. All your responses must be one sentence long.\"\"\"},    \n",
    "{'role':'user',\n",
    " 'content':\"\"\"write me a story about a happy carrot\"\"\"},\n",
    "] \n",
    "response = get_completion_from_messages(messages, \n",
    "                                        temperature =1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb73cd1b",
   "metadata": {},
   "source": [
    "### We can also output the summarized analysis of the output\n",
    "- Note that the way that it's defined the tokens is various."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c1763f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion_and_token_count(messages, \n",
    "                                   model=\"gpt-3.5-turbo\", \n",
    "                                   temperature=0, \n",
    "                                   max_tokens=500):\n",
    "    \"\"\"Function that generate results and provide the count tokens as well\"\"\"\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature, \n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "    \n",
    "    content = response.choices[0].message[\"content\"]\n",
    "    \n",
    "    # Get the number of token in the prompt, output, and total of both\n",
    "    token_dict = {\n",
    "        'prompt_tokens':response['usage']['prompt_tokens'],\n",
    "        'completion_tokens':response['usage']['completion_tokens'],\n",
    "        'total_tokens':response['usage']['total_tokens'],\n",
    "    }\n",
    "\n",
    "    return content, token_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1088f559",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "{'role':'system', \n",
    " 'content':\"\"\"You are an assistant who responds\\\n",
    " in the style of Dr Seuss.\"\"\"},    \n",
    "{'role':'user',\n",
    " 'content':\"\"\"write me a very short poem \\ \n",
    " about a happy carrot\"\"\"},  \n",
    "] \n",
    "response, token_dict = get_completion_and_token_count(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9bd51358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a garden so bright, a carrot did grow,\n",
      "With a smile on its face, a happy little glow.\n",
      "It danced in the sun, and wiggled its toes,\n",
      "Oh, what a joyous veggie, how it happily grows!\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "10f1a693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt_tokens': 37, 'completion_tokens': 50, 'total_tokens': 87}\n"
     ]
    }
   ],
   "source": [
    "print(token_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a506b97f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
