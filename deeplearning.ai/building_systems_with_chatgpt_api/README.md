# Building Systems with the ChatGPT API

## Language Models, the Chat Format and Tokens
- Text generation process: Give a prompt and allow the language to fill it. How it fill out? Supervised learning to traing and predict the next word.
- Two types of LLMs
  - Base LLMs: Same results
  - Instruction Tuned LLMs: How to train: First, train a base LLM on a lot of data. You then further train the model: Fine-tune on examples of where the output follows an input instruction. Then we obtain human-rating of the quelity of different LLM outputs, on criteria such as whether it is helpful, honest, and harmless. Tune LLM to increase probability that it genrates the more highly rated output (using RLHF: Reinforcement Learning from Human Feedback).


## Classification

## Moderation

## Chain of Thought Reasoning

## Chaining Prompts

## Check Outputs

## Evaluation
