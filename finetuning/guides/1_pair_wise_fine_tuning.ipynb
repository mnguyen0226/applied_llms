{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Pair-based fine-tuning\n",
    "- Minh Nguyen\n",
    "- 11/13/2024\n",
    "\n",
    "Fine-tuning is about adapting a pre-trained model to a specific task by training it further on task-specific data.\n",
    "- Dataset: AllNLI (Natural Language Inference) provides pairs of sentences labeld as entailments (similar), neutral, or contradiction (dissimilar).\n",
    "- Task: Sentence similarity, where the goal is to predict how semantically similar 2 setences are.\n",
    "- Loss Function: Pair-based loss like CosineSimilarityLoss, which optimizes the model to produce embeddings closer for similar pairs and farther apart for dissimilar pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Load the dataset\n",
    "- The dataset contains pairs of sentences (premise, hypothesis) and a label:\n",
    "    - label = 2: entailment (similar)\n",
    "    - label = 1: neutral (partially related)\n",
    "    - label = 0: contradict (dissimilar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mnguyen0226/anaconda3/envs/fingpt/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples: 1000\n",
      "Validation examples: 200\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the AllNLI dataset\n",
    "dataset = load_dataset(\"sentence-transformers/all-nli\", \"pair-class\")\n",
    "\n",
    "# Shuffle the dataset and select a random subset\n",
    "train_dataset = dataset[\"train\"].shuffle(seed=42).select(range(1000))  # Random 10,000 examples\n",
    "eval_dataset = dataset[\"test\"].shuffle(seed=42).select(range(200))     # Random 2,000 examples\n",
    "\n",
    "# Check the sizes\n",
    "print(f\"Training examples: {len(train_dataset)}\")\n",
    "print(f\"Validation examples: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the data: Convert entailment to 1, contradict to 0, and neutral to 0.5\n",
    "\n",
    "# Map labels to similarity sores\n",
    "def map_labels_to_scores(example):\n",
    "    if example['label'] == 2: example['score'] = 1.0\n",
    "    elif example['label'] == 0: example['score'] = 0.0\n",
    "    else: example['score'] = 0.5\n",
    "    return example\n",
    "\n",
    "# Apply mapping\n",
    "train_dataset = train_dataset.map(map_labels_to_scores)\n",
    "eval_dataset = eval_dataset.map(map_labels_to_scores)\n",
    "\n",
    "# Keep only relevant columns\n",
    "train_dataset = train_dataset.select_columns(['premise', 'hypothesis', 'score'])\n",
    "eval_dataset = eval_dataset.select_columns(['premise', 'hypothesis', 'score'])\n",
    "\n",
    "print(len(train_dataset))\n",
    "print(len(eval_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to InputExample objects required by Sentence Transformers\n",
    "from sentence_transformers import InputExample\n",
    "\n",
    "# Convert to InputExample format\n",
    "train_examples = [\n",
    "    InputExample(\n",
    "        texts=[row['premise'], row['hypothesis']],\n",
    "        label=float(row['score'])\n",
    "    )\n",
    "    for row in train_dataset    \n",
    "]\n",
    "\n",
    "# Convert to InputExample format\n",
    "eval_examples = [\n",
    "    InputExample(\n",
    "        texts=[row['premise'], row['hypothesis']],\n",
    "        label=float(row['score'])\n",
    "    )\n",
    "    for row in eval_dataset    \n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch data loader\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
    "eval_dataloader = DataLoader(eval_examples, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Pre-trained Model\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.losses import CosineSimilarityLoss\n",
    "\n",
    "# Define the loss function Cosine Similarity\n",
    "loss = CosineSimilarityLoss(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [01:12<00:00,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 72.6376, 'train_samples_per_second': 13.767, 'train_steps_per_second': 0.867, 'train_loss': 0.2825991009909009, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "\n",
    "# Create an evaluator\n",
    "evaluator = EmbeddingSimilarityEvaluator(\n",
    "    sentences1=[example.texts[0] for example in eval_examples],\n",
    "    sentences2=[example.texts[1] for example in eval_examples],\n",
    "    scores=[example.label for example in eval_examples]\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, loss)],\n",
    "    evaluator=evaluator,\n",
    "    evaluation_steps=100,  # Evaluate every 100 steps\n",
    "    epochs=1,  # Increase epochs for better performance\n",
    "    warmup_steps=100,  # Use warmup for stability\n",
    "    output_path=\"output/fine_tuned_allnli_model_1\",  # Save path\n",
    "    save_best_model=True  # Save only the best-performing model\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson Correlation: -0.5711\n",
      "Spearman Correlation: -0.5625\n"
     ]
    }
   ],
   "source": [
    "# Evaluate fine-tune model\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def evaluate_model(model, dataset):\n",
    "    embeddings1 = model.encode(dataset['premise'], batch_size=16, convert_to_tensor=True)\n",
    "    embeddings2 = model.encode(dataset['hypothesis'], batch_size=16, convert_to_tensor=True)\n",
    "    \n",
    "    # Compute pairwise cosine similarity\n",
    "    similarities = cosine_similarity(embeddings1.cpu(), embeddings2.cpu()).diagonal()\n",
    "    return similarities\n",
    "\n",
    "# Evaluate the model\n",
    "predicted_scores = evaluate_model(model, eval_dataset)\n",
    "ground_truth_scores = eval_dataset['score']\n",
    "\n",
    "pearson_corr = pearsonr(predicted_scores, ground_truth_scores)\n",
    "spearman_corr = spearmanr(predicted_scores, ground_truth_scores)\n",
    "\n",
    "print(f\"Pearson Correlation: {pearson_corr[0]:.4f}\")\n",
    "print(f\"Spearman Correlation: {spearman_corr[0]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the capital of France?\n",
      "Most Relevant Sentence: Paris is the capital of France.\n"
     ]
    }
   ],
   "source": [
    "# Load the fine-tuned model\n",
    "fine_tuned_model = SentenceTransformer(\"output/fine_tuned_allnli_model_1\")\n",
    "\n",
    "# Query and corpus\n",
    "query = \"What is the capital of France?\"\n",
    "corpus = [\n",
    "    \"Paris is the capital of France.\",\n",
    "    \"France is a country in Europe.\",\n",
    "    \"The Eiffel Tower is located in Paris.\",\n",
    "    \"London is the capital of the UK.\"\n",
    "]\n",
    "\n",
    "# Encode query and corpus\n",
    "query_embedding = fine_tuned_model.encode([query])\n",
    "corpus_embeddings = fine_tuned_model.encode(corpus)\n",
    "\n",
    "# Compute similarities\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarities = cosine_similarity(query_embedding, corpus_embeddings).flatten()\n",
    "most_relevant_idx = similarities.argmax()\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Most Relevant Sentence: {corpus[most_relevant_idx]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fingpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
